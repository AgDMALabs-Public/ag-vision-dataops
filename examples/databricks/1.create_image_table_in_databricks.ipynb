{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create a delta table in Databricks\n",
    "* Find all the files of interest for that table.\n",
    "* Generate a pandas df.\n",
    "* Save the table with spark."
   ],
   "id": "2f36b5875b912293"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": "%pip install git+https://github.com/AgDMALabs-Public/ag-vision-dataops.git",
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from ag_vision.pipelines import image_processing as ip\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "id": "21c12539163feda3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set to True if you want to generate the whole table from scratch. (this will read all metadatafile)\n",
    "# Set to False if you only want to add new data. (this will read only metadata files for images not in the table.)\n",
    "REGEN = True\n",
    "TABLE_NAME = \"use1_prod_artemis_catalog_3718194974443840.production.images_table\""
   ],
   "id": "fc91a1f44733d419",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load the root directory\n",
    "root_path = \"/Volumes/use1_prod_artemis_catalog_3718194974443840/production/data/\"\n",
    "\n",
    "df = spark.read.format(\"binaryFile\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(root_path)\n",
    "\n",
    "# 2. Filter by Depth AND Regex\n",
    "# Your pattern: artemis/*/*/*/*/*/*/im*/**/*\n",
    "# This implies a specific number of folders between 'artemis' and the file.\n",
    "img_paths_df = df.select(\"path\").filter(\n",
    "    # Ensure it's an image\n",
    "    F.col(\"path\").rlike(r\"\\.(jpg|jpeg|png|webp)$\") &\n",
    "    # Ensure 'im' folder is in the path\n",
    "    F.col(\"path\").contains(\"/im\") &\n",
    "    # Ensure the depth matches your 13-slash pattern, this will make sure annotation images done make it in.\n",
    "    (F.size(F.split(F.col(\"path\"), \"/\")) >= 13)\n",
    ")\n",
    "\n",
    "# 3. Collect\n",
    "img_paths_list = [row.path for row in img_paths_df.toLocalIterator()]\n",
    "\n",
    "print(f\"Found {len(img_paths_list)} paths at the correct depth.\")"
   ],
   "id": "6541e84e45937c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "old_df = spark.table(TABLE_NAME).toPandas()\n",
    "print(f\"The Old Table len is {len(old_df)}\")"
   ],
   "id": "3afbd2cb38338c06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "img_paths_list = [x.replace('dbfs:', '') for x in img_paths_list]\n",
    "if not REGEN:\n",
    "    s = pd.Series(img_paths_list)\n",
    "    run_list = s[~s.isin(old_df['file_path'])].tolist()\n",
    "else:\n",
    "    run_list = img_paths_list"
   ],
   "id": "136cc6a7efc59ced",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(f\"The len of the run list is. {len(run_list)}\")",
   "id": "7ca502a68606facb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Need to generate the schema.\n",
    "sample_df = ip.generate_images_table(img_list=run_list[:1],\n",
    "                                      platform='db',\n",
    "                                      project_index=6)\n",
    "\n",
    "my_schema = spark.createDataFrame(sample_df).schema.simpleString()\n",
    "\n",
    "df = spark.createDataFrame([(i,) for i in run_list], [\"item\"])"
   ],
   "id": "8bc3991d6cc24b5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_batch(iterator):\n",
    "    \"\"\"\n",
    "    Processes an iterator of pandas DataFrames (chunks) and yields processed results.\n",
    "    \"\"\"\n",
    "    for pdf in iterator:\n",
    "        # pdf is a pandas DataFrame chunk\n",
    "        items = pdf[\"item\"].tolist()\n",
    "\n",
    "        if not items:\n",
    "            # Yield an empty DataFrame with the correct columns if the chunk is empty\n",
    "            yield pd.DataFrame(columns=sample_df.columns)\n",
    "        else:\n",
    "            # Process the items and yield the resulting DataFrame\n",
    "            yield ip.generate_images_table(img_list=items,\n",
    "                                           platform='db',\n",
    "                                           project_index=6)\n",
    "\n",
    "img_spark_df = df.mapInPandas(process_batch, schema=my_schema)"
   ],
   "id": "7061362b24b9c6b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Check if the table exists and REGEN is False\n",
    "table_exists = spark.catalog.tableExists(TABLE_NAME)\n",
    "\n",
    "if not REGEN and table_exists:\n",
    "    # Load the existing data as a Spark DataFrame\n",
    "    old_spark_df = spark.read.table(TABLE_NAME)\n",
    "    old_spark_df = old_spark_df.drop(\"image_path\", \"error\")\n",
    "\n",
    "    # Combine (Union) the old data with the new results\n",
    "    # unionByName is safer as it matches columns by name, not position\n",
    "    final_spark_df = old_spark_df.unionByName(img_spark_df)\n",
    "else:\n",
    "    # If REGEN is True or table doesn't exist, just use the new data\n",
    "    final_spark_df = img_spark_df\n",
    "\n",
    "#2. Write the final result back to the table\n",
    "final_spark_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(TABLE_NAME)"
   ],
   "id": "ac68aa38fe1c2dd6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
